{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone - Restaurant Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a recommender system for Melbourne cafes/restaurants.\n",
    "\n",
    "\n",
    "The Data:\n",
    "- Source: Zomato\n",
    "- URL: https://www.zomato.com/melbourne/great-food-no-bull\n",
    "- Best of Melbourne Collection\n",
    "- Retrieved 250 restaurants, and a max of 200 reviews per restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from math import sqrt\n",
    "from datetime import date\n",
    "from time import sleep\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape business profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the url's of the businesses featured in 'best of Melbourne' collection \n",
    "def get_business_urls(html):\n",
    "    \n",
    "    # restaurant url's\n",
    "    try:\n",
    "        url_list=Selector(text=html).xpath('//div[@class=\"row col-res-list collection_listings_container\"]/div/div/div/a/@href').extract()\n",
    "    except:\n",
    "        url_list=np.NaN\n",
    "        return np.NaN\n",
    "    \n",
    "    print(url_list)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reviews for a specific business\n",
    "def get_reviews_per_url(html):\n",
    "\n",
    "    reviews_per_business = []\n",
    "    review_index = 0\n",
    "    \n",
    "    # get restaurant id and review id (separately, as these two are outside the xpath scope defined below)\n",
    "    # review id list \n",
    "    try:\n",
    "        review_id_list = Selector(text=html).xpath('//div[@id=\"reviews-container\"]/*//div[@data-snippet=\"restaurant-review\"]/@data-review_id').extract()\n",
    "        print('# of review ids: ',len(review_id_list))\n",
    "    except:\n",
    "        review_id_list= np.NaN\n",
    "        print('error on review id list for : ', review_id_list)\n",
    "        \n",
    "    # restaurant id\n",
    "    try:\n",
    "        restaurant_id = Selector(text=html).xpath('//div[@id=\"reviews-container\"]/*//div[@data-snippet=\"restaurant-review\"]/@data-res_id').extract()[0]\n",
    "    except:\n",
    "        restaurant_id = np.NaN\n",
    "        print('error on restaurant_id for : ', restaurant_id)\n",
    "        \n",
    "    # get data for displayed reviews \n",
    "    visible_reviews_full = Selector(text=html).xpath('//div[@id=\"reviews-container\"]/*//div[@data-snippet=\"restaurant-review\"]/div[contains(@class,\"ui segment\")]').extract()\n",
    "    num_reviews=len(visible_reviews_full)\n",
    "\n",
    "    # iterate through the displayed reviews\n",
    "    for review in visible_reviews_full:\n",
    "\n",
    "        # review id\n",
    "        try:\n",
    "            review_id= review_id_list[review_index]\n",
    "        except:\n",
    "            print('error on iterating through review id for: ', review_id_list)\n",
    "            \n",
    "        # user id\n",
    "        try:\n",
    "            user_id=Selector(text=review).xpath('//div[contains(@class,\"content col-l-11\")]/*/a[contains(@href,\"https://www.zomato.com\")]/@data-entity_id').extract()[0]\n",
    "        except:\n",
    "            user_id=np.NaN\n",
    "\n",
    "        # user name\n",
    "        try:\n",
    "            user_name=Selector(text=review).xpath('//div[contains(@class,\"content col-l-11\")]/*/a[contains(@href,\"https://www.zomato.com\")]/text()').extract()[0].strip()\n",
    "        except:\n",
    "            user_name=np.NaN\n",
    "\n",
    "        # no. of reviews by user\n",
    "        try:\n",
    "            user_review_count=Selector(text=review).xpath('//div[contains(@class,\"content col-l-11\")]/span[contains(@class,\"grey-text\")]/text()').extract()[0].strip().split(',')[0].strip().split(' ')[0]\n",
    "        except:\n",
    "            user_review_count=np.NaN\n",
    "\n",
    "        # no. of followers of user\n",
    "        try:\n",
    "            user_followers_count=Selector(text=review).xpath('//div[contains(@class,\"content col-l-11\")]/span[contains(@class,\"grey-text\")]/text()').extract()[0].strip().split(',')[1].strip().split(' ')[0]\n",
    "        except:\n",
    "            user_followers_count=np.NaN\n",
    "\n",
    "        # review date\n",
    "        try:\n",
    "            review_date=Selector(text=review).xpath('//time/@datetime').extract()[0]\n",
    "        except:\n",
    "            review_date=np.NaN\n",
    "\n",
    "        # star rating\n",
    "        try:\n",
    "            star_rating=Selector(text=review).xpath('//div[contains(@class,\"rev-text\")]/div[contains(@class,\"ttupper fs12px\")]/@aria-label').extract()[0].split(' ')[1]\n",
    "        except:\n",
    "            star_rating=np.NaN\n",
    "\n",
    "        # textual review\n",
    "        try:\n",
    "            textual_review=Selector(text=review).xpath('//div[contains(@class,\"rev-text \")]/text()').extract()[1:]\n",
    "        except:\n",
    "            textual_review=np.NaN\n",
    "\n",
    "        # clean textual review, convert to string\n",
    "        try:\n",
    "            textual_review[0] = textual_review[0].replace('\\xa0\\n', '').strip()\n",
    "            textual_review = ''.join([line.strip() for line in textual_review if line.strip() is not ''])\n",
    "        except:\n",
    "            print('error in cleaning textual_review for : ', textual_review)\n",
    "            \n",
    "        # append scraped review to list\n",
    "        reviews_per_business.append([review_id, restaurant_id, user_id, user_name, user_review_count, user_followers_count, review_date, star_rating, textual_review])\n",
    "        review_index += 1\n",
    "        \n",
    "    return reviews_per_business, num_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of code for getting the business url's in the 'best of Melbourne' collection\n",
    "\n",
    "# open business reviews url\n",
    "main_url = 'https://www.zomato.com/melbourne/great-food-no-bull'\n",
    "driver = webdriver.Chrome(executable_path=\"chromedriver.exe\")\n",
    "try:\n",
    "    sleep(5)\n",
    "    driver.get(main_url)\n",
    "except:\n",
    "    print('error in loading driver for business url: ', main_url)        \n",
    "assert \"Zomato\" in driver.title\n",
    "\n",
    "# parse first results page\n",
    "sleep(5)\n",
    "# set HTML to the response from the http request\n",
    "html = driver.page_source\n",
    "\n",
    "url_list = get_business_urls(html)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of code for getting the reviews for each business featured in 'best of Melbourne' collection\n",
    "\n",
    "# target count of reviews to extract\n",
    "review_target_count = 200\n",
    "all_business_reviews = []\n",
    "num_reviews_list = []\n",
    "business_with_zero_reviews = []\n",
    "\n",
    "monitor_count=0    \n",
    "\n",
    "total_count=len(url_list)\n",
    "# iterate through the url's of the businesses to extract the reviews\n",
    "for business_url in url_list:\n",
    "\n",
    "    # open business reviews url\n",
    "    driver = webdriver.Chrome(executable_path=\"chromedriver.exe\")\n",
    "    try:\n",
    "        driver.get(business_url + '/reviews')\n",
    "    except:\n",
    "        print('error in loading driver for business url: ', business_url)\n",
    "        \n",
    "    assert \"Zomato\" in driver.title\n",
    "\n",
    "    # if there's a Popular (Reviews) section, change and click on All Reviews section\n",
    "    driver = switch_from_Popular_to_All_Reviews(driver, business_url + '/reviews')\n",
    "    \n",
    "    # display more reviews (in preparation for scraping the target # of reviews)\n",
    "    driver, visible_review_count = display_more_reviews(driver, review_target_count)\n",
    "\n",
    "    # get final count of visible reviews (for this particular business)\n",
    "    html = driver.page_source\n",
    "\n",
    "    # if review list is not empty\n",
    "    if visible_review_count > 0:\n",
    "        # scrape the displayed reviews\n",
    "        reviews_per_business, num_reviews = get_reviews_per_url(html)\n",
    "\n",
    "        num_reviews_list.append(num_reviews)\n",
    "        \n",
    "        # add reviews per business to overall list of business reviews\n",
    "        all_business_reviews += reviews_per_business\n",
    "\n",
    "    # businesses with zero reviews\n",
    "    else:\n",
    "        business_with_zero_reviews.append(business_url)\n",
    "\n",
    "    print('url: ', business_url)    \n",
    "    monitor_count += 1\n",
    "    print(str(monitor_count), ' of ', str(total_count), \" businesses completed...\")\n",
    "\n",
    "    # save reviews for every 20 businesses to a csv file\n",
    "    if (monitor_count % 20) == 0:\n",
    "        reviews_20 = pd.DataFrame(all_business_reviews)\n",
    "        # save the latest 20 businesses\n",
    "        reviews_20.to_csv('./datasets/Reviews - Best of Melbourne ' + str(monitor_count-20) + ' to ' + str(monitor_count-1) +  '.csv')\n",
    "        # clear business reviews list to make way for the next 20 businesses\n",
    "        all_business_reviews = []\n",
    "    \n",
    "    # close the Chrome webdriver\n",
    "    driver.close()\n",
    "    \n",
    "print('business with zero reviews: ', business_with_zero_reviews)\n",
    "print('number of reviews: ', num_reviews_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Business Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# display more reviews by clicking on 'Load More' element\n",
    "def display_more_reviews(driver, review_target_count):\n",
    "    \n",
    "    html = driver.page_source\n",
    "    \n",
    "    try:\n",
    "        # get the visible reviews and review count (for this business)\n",
    "        visible_reviews = Selector(text=html).xpath('//div[@id=\"reviews-container\"]/*//div[@data-snippet=\"restaurant-review\"]/div[contains(@class,\"ui segment\")]').extract()    \n",
    "        visible_review_count = len(visible_reviews)\n",
    "\n",
    "        # open more reviews by clicking on Load More until the desired # of reviews (for this particular business) is displayed\n",
    "        while visible_review_count < review_target_count:\n",
    "            try:\n",
    "                # access and click on Load More element - to extend the list of displayed reviews\n",
    "                load_more_element = driver.find_element_by_xpath('//div[@class=\"load-more bold ttupper tac cursor-pointer fontsize2\"]/span[contains(@class,\"zred\")]')\n",
    "                load_more_element.click()\n",
    "                sleep(5)\n",
    "\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            # update the count of visible reviews (for this particular business)\n",
    "            html = driver.page_source\n",
    "            visible_reviews = Selector(text=html).xpath('//div[@id=\"reviews-container\"]/*//div[@data-snippet=\"restaurant-review\"]/div[contains(@class,\"ui segment\")]').extract()\n",
    "            visible_review_count = len(visible_reviews)\n",
    "    except:\n",
    "        print('ERROR IN DISPLAY_MORE_REVIEWS')\n",
    "    \n",
    "    return driver, visible_review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch from Popular Reviews to All Reviews, if applicable\n",
    "def switch_from_Popular_to_All_Reviews(driver, url):\n",
    "    \n",
    "    html = driver.page_source\n",
    "    try:\n",
    "        # see if Popular Reviews tab exists\n",
    "        popular_section = Selector(text=html).xpath('//div[@id=\"selectors\"]/a[contains(@data-sort,\"reviews-top\")]/text()').extract()[0]\n",
    "        try:\n",
    "            # access and click on All Reviews element - to switch from Popular Reviews list\n",
    "            load_more_element = driver.find_element_by_xpath('//div[@id=\"selectors\"]/a[contains(@data-sort,\"reviews-dd\")]')\n",
    "            sleep(3)\n",
    "            load_more_element.click()\n",
    "            sleep(1)\n",
    "            print('Popular Reviews section exists for: ' + url)\n",
    "\n",
    "        except:\n",
    "            display('All Reviews element not found, or click not successful for:' + url)\n",
    "    \n",
    "    except:\n",
    "        print('No Popular Reviews section for: ' + url)\n",
    "        \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all subset csv's for the 'best of Melbourne' reviews into 1 csv, 'reviews - best of Melbourne.csv'\n",
    "csv_list = []\n",
    "reviews_df = pd.DataFrame()\n",
    "\n",
    "import os\n",
    "# create list of all review csv's\n",
    "for file in os.listdir():\n",
    "    if file.startswith(\"Reviews - Best of Melbourne\"):\n",
    "        csv_list.append(file)\n",
    "\n",
    "# load csv's on to a dataframe\n",
    "for csv in csv_list:\n",
    "    reviews_df_per_csv = pd.read_csv(csv)\n",
    "    reviews_df = pd.concat([reviews_df, reviews_df_per_csv])\n",
    "\n",
    "# save entire dataframe to a single csv for backup\n",
    "reviews_df.columns=['per_csv_id','review_id', 'restaurant_id', 'user_id', 'user_name', 'user_review_count', 'user_followers_count', 'review_date', 'star_rating', 'textual_review']\n",
    "reviews_df.to_csv('./datasets/reviews - best of Melbourne.csv')\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract restaurant details through Zomato API \n",
    "# This has to be done via Zomato API to include the latitude/longitude details of the businesses in 'best of Melbourne' collection\n",
    "\n",
    "#api_key = <zomato-api-here>\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# using restaurant id's in the extracted reviews, retrieve the restaurant details via Zomato API\n",
    "def extract_businesses_details(restaurant_id_list):\n",
    "    \n",
    "    # set headers\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'user-key': api_key\n",
    "    }\n",
    "\n",
    "    businesses_details_df = pd.DataFrame()\n",
    "    \n",
    "    # for every restaurant_id in the Zomato businesses dataset\n",
    "    for restaurant_id in restaurant_id_list:\n",
    "        response = requests.get('https://developers.zomato.com/api/v2.1/restaurant?res_id=' + str(restaurant_id), headers=headers)\n",
    "        response_df = pd.DataFrame([response.json()])\n",
    "        businesses_details_df = pd.concat([businesses_details_df,response_df], ignore_index=True)\n",
    "\n",
    "    # pickle dataset\n",
    "    save_pickle = open('./datasets/Zomato - Best of Melbourne.pkl',\"wb\")\n",
    "    pickle.dump(businesses_details_df, save_pickle)\n",
    "    save_pickle.close()\n",
    "    \n",
    "    return businesses_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load onto a dataframe the 'best of Melbourne' business profiles\n",
    "restaurant_id_list = reviews_df.restaurant_id.unique()\n",
    "businesses_df = extract_businesses_details(restaurant_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Businesses and Reviews dataframes (post-scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load businesses and reviews onto their respective dataframes \n",
    "businesses_df = pd.DataFrame()\n",
    "reviews_df = pd.DataFrame()\n",
    "# load the csv of the scraped businesses on to a dataframe\n",
    "businesses_df = pd.read_pickle('./datasets/Zomato - Best of Melbourne.pkl')\n",
    "# load the csv of the scraped reviews on to a dataframe\n",
    "reviews_df = pd.read_csv('./datasets/reviews - best of Melbourne - clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first (unnamed) column\n",
    "reviews_df = reviews_df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename id, and convert to float\n",
    "businesses_df.rename({'id': 'restaurant_id'},axis=1,inplace=True)\n",
    "businesses_df.loc[:,'restaurant_id'] = businesses_df.restaurant_id.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sns_plot(plot_type='scatter', x=None, y=None, data=None, xlabel='_', ylabel='_', title='_', bins=20, figsize=(10,5)):\n",
    "\n",
    "    # plot bar chart\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.gca()\n",
    "    if plot_type == 'dist':\n",
    "        sns.distplot(x,bins=bins, ax=ax)\n",
    "    elif plot_type == 'scatter':\n",
    "        plt.scatter(x=x,y=y,alpha=0.6, edgecolors='w')\n",
    "#        sns.pointplot(x=x,y=y,ax=ax)\n",
    "#    elif plot_type == 'joint':\n",
    "#        sns.jointplot(x=x, y=y, data=data, kind='reg', space=0, size=5, ratio=4)\n",
    "    ax.set_xlabel(xlabel, fontsize=18)\n",
    "    ax.set_ylabel(ylabel, fontsize=19)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total # of unique restaurants, users, and reviews in the dataset\n",
    "pd.DataFrame([[len(businesses_df.restaurant_id.unique())], [len(reviews_df.user_id.unique())], [reviews_df.review_id.count()]], columns=['Total'], index = ['# of Restaurants', '# of Users', '# of Reviews']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list businesses that have duplicates in the dataset\n",
    "businesses_df[businesses_df.restaurant_id.isin(businesses_df.restaurant_id.value_counts()[businesses_df.restaurant_id.value_counts()>1].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the # of reviews per restaurant\n",
    "sns_plot(plot_type='dist', x=reviews_df.restaurant_id.value_counts(), xlabel='# of Reviews', ylabel='Frequency', title='# of Reviews Per Restaurant', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the # of reviews given per restaurant, when the # of reviews is less than or equal to 50\n",
    "sns_plot(plot_type='dist', x=reviews_df.restaurant_id.value_counts()[reviews_df.restaurant_id.value_counts()<=50], xlabel='# of Reviews', ylabel='Frequency', title='# of Reviews Per Restaurant\\nReviews Per Restaurant <= 50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_df.groupby('restaurant_id')['restaurant_id'].count().sort_values()\n",
    "reviews_df[reviews_df.restaurant_id==16585728]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of reviews per restaurant\n",
    "review_count_per_restaurant = reviews_df.groupby('restaurant_id')['restaurant_id'].count()\n",
    "review_count_per_restaurant.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile range of review count per restaurant \n",
    "review_count_per_restaurant.quantile(np.arange(0,1,.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of restaurants with total reviews = 25\n",
    "(review_count_per_restaurant==200).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the # of reviews given per restaurant, when the # of reviews is greater than 50\n",
    "sns_plot(plot_type='dist', x=reviews_df.restaurant_id.value_counts()[reviews_df.restaurant_id.value_counts()>50], xlabel='# of Reviews', ylabel='Frequency', title='# of Reviews Per Restaurant\\nReviews Per Restaurant > 50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[reviews_df.star_rating.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set review_date column to datetime object\n",
    "reviews_df.loc[:,'review_date'] = pd.to_datetime(reviews_df.review_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the reviews with no star ratings - these reviews date back to the 2014-2015 period, thus, relevance-wise might be worth dropping\n",
    "reviews_df.drop(index=reviews_df[reviews_df.star_rating.isnull()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the votes column - votes column as per research is not user-generated and instead derived out of an algorithm that's exclusive to Zomato. Thus, it's worth removing.\n",
    "#businesses_df.drop(columns=['votes'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change user_followers_count from NaN to zero\n",
    "reviews_df.loc[reviews_df.user_followers_count.isnull(), 'user_followers_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change non-numeric values in price column to zero\n",
    "# businesses_df.loc[businesses_df.price == '[]', 'price'] = '[0]'\n",
    "# businesses_df.price = businesses_df.price.str.replace(r\"[A$\\'\\[\\]]\",'').astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change non-numeric values in ratings column to zero\n",
    "# # first change non-numeric to NaN\n",
    "# businesses_df.overall_rating = pd.to_numeric(businesses_df.overall_rating, errors='coerce')\n",
    "# # then change NaN to zero\n",
    "# businesses_df.overall_rating.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tally no. of restaurants with 1 review , and with less than 5 reviews\n",
    "review_count_per_restaurant = reviews_df.groupby('restaurant_id')['restaurant_id'].count()\n",
    "pd.DataFrame([(review_count_per_restaurant==1).sum(), (review_count_per_restaurant<5).sum()], columns=['Total'], index=['Restaurants with only 1 review', 'with less than 5 reviews']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after above data preprocessing, show updated # of unique restaurants, users, and reviews in the dataset\n",
    "pd.DataFrame([[len(businesses_df.restaurant_id.unique())], [len(reviews_df.user_id.unique())], [reviews_df.review_id.count()]], columns=['Total'], index = ['# of Restaurants', '# of Users', '# of Reviews']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dataframe as csv for backup reflecting all changes to this point\n",
    "# businesses_df.to_pickle('./datasets/Zomato - All Cafes in Melbourne - pass 1.pkl')\n",
    "# # save dataframe as for backup reflecting all changes to this point\n",
    "# reviews_df.to_pickle('./datasets/reviews - clean - pass 1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------\n",
    "#### Load from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# businesses_df = pd.DataFrame()\n",
    "# reviews_df = pd.DataFrame()\n",
    "# # load the csv of the scraped businesses on to a dataframe\n",
    "# businesses_df = pd.read_pickle('./datasets/Zomato - All Cafes in Melbourne - pass 1.pkl')\n",
    "# # load the csv of the scraped reviews on to a dataframe\n",
    "# reviews_df = pd.read_pickle('./datasets/reviews - clean - pass 1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution for the # of reviews given per user\n",
    "sns_plot(plot_type='dist', x=reviews_df.user_id.value_counts(), xlabel='# of Reviews', ylabel='Frequency', title='# of Reviews Per User', bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution for the # of reviews given per user, when the # of reviews is less than or equal to 50\n",
    "sns_plot(plot_type='dist', x=reviews_df.user_id.value_counts()[reviews_df.user_id.value_counts()<=50], xlabel='# of Reviews', ylabel='Frequency', title='# of Reviews Per User\\nReviews Per User <= 50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distribution for the # of reviews given per user, when the # of reviews is greater than 50\n",
    "sns_plot(plot_type='dist', x=reviews_df.user_id.value_counts()[reviews_df.user_id.value_counts()>50], xlabel='# of Reviews', ylabel='Frequency', title='# of Reviews Per User\\nReviews Per User > 50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter reviews dataset so that it only shows the 2018 reviews\n",
    "reviews_df_2018 = reviews_df[reviews_df.review_date.dt.year == 2018]\n",
    "reviews_df_2018 = reviews_df_2018[['restaurant_id','review_date', 'star_rating']]\n",
    "reviews_df_2018.set_index('review_date',inplace=True)\n",
    "reviews_df_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "businesses_df.name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# latest 2018 ratings for a given restaurant \n",
    "rest_id = 16578793 # Patricia Coffee Brewers\n",
    "rest_name = businesses_df[businesses_df.restaurant_id==rest_id]['name'].values[0]\n",
    "per_restaurant_ratings_timeseries = reviews_df_2018[reviews_df_2018.restaurant_id==rest_id]\n",
    "per_restaurant_ratings_timeseries.index = per_restaurant_ratings_timeseries.index.date\n",
    "# plot star_rating for a particular restaurant\n",
    "sns_plot(plot_type='scatter', x=per_restaurant_ratings_timeseries.index, y=per_restaurant_ratings_timeseries.star_rating, \n",
    "         xlabel='Date', ylabel='Rating', title=rest_name + '\\n' + 'Star Ratings for 2018', figsize=(15,5))\n",
    "#per_restaurant_ratings_timeseries.sort_index().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the star ratings for a restaurant in 2018\n",
    "sns_plot(plot_type='dist', x=per_restaurant_ratings_timeseries.star_rating, xlabel='Star Ratings', ylabel='Frequency', title=rest_name + '\\nStar Ratings Distribution for 2018',bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest 2018 ratings for a given restaurant \n",
    "rest_id = 16577492 # Chin Chin\n",
    "rest_name = businesses_df[businesses_df.restaurant_id==rest_id]['name'].values[0]\n",
    "per_restaurant_ratings_timeseries = reviews_df_2018[reviews_df_2018.restaurant_id==rest_id]\n",
    "per_restaurant_ratings_timeseries.index = per_restaurant_ratings_timeseries.index.date\n",
    "# plot star_rating for a particular restaurant\n",
    "sns_plot(plot_type='scatter', x=per_restaurant_ratings_timeseries.index, y=per_restaurant_ratings_timeseries.star_rating, \n",
    "         xlabel='Date', ylabel='Rating', title=rest_name + '\\n' + 'Star Ratings for 2018', figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the star ratings for a restaurant in 2018\n",
    "sns_plot(plot_type='dist', x=per_restaurant_ratings_timeseries.star_rating, xlabel='Star Ratings', ylabel='Frequency', title=rest_name + '\\nStar Ratings Distribution for 2018',bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation between a user's # of reviews IN the dataset and the user's average rating\n",
    "review_count_per_user = reviews_df.groupby('user_id')['review_id'].count()\n",
    "average_rating_per_user = reviews_df.groupby('user_id')['star_rating'].mean()\n",
    "average_rating_review_count = pd.merge(pd.DataFrame(average_rating_per_user), pd.DataFrame(review_count_per_user), left_on='user_id', right_on='user_id')\n",
    "sns_plot(plot_type='scatter', x=average_rating_review_count.review_id, y=average_rating_review_count.star_rating,\n",
    "         xlabel='Total # of Reviews Per User', ylabel='Average Rating', title='User Review Count vs. Average Rating\\nReview Count - total within Dataset', figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation between a user's # of reviews (profiled as overall total in Zomato) and the user's average rating\n",
    "average_rating_per_user = pd.DataFrame(reviews_df.groupby('user_id')['star_rating'].mean())\n",
    "average_rating_per_user['total_review_count'] = average_rating_per_user.index.map(lambda x: np.max(reviews_df[reviews_df.user_id == x]['user_review_count']))\n",
    "sns_plot(plot_type='scatter', x=average_rating_per_user.total_review_count, y=average_rating_per_user.star_rating,\n",
    "         xlabel='Total # of Reviews Per User', ylabel='Average Rating', title=\"User Review Count vs. Average Rating\\nReview Count - User's Total in Zomato\", figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the average star ratings of all restaurants in the dataset\n",
    "rating_mean_df = reviews_df.groupby('restaurant_id')['star_rating'].mean()\n",
    "sns_plot(plot_type='dist', x=rating_mean_df, xlabel='Average Star Ratings per Restaurant', ylabel='Frequency', title=\"Average Star Ratings Distribution\\n All Restaurants in 'Best of Melbourne' collection\",bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the average followers count of all users in the dataset\n",
    "rating_mean_df = reviews_df.groupby('user_id')['user_followers_count'].mean()\n",
    "sns_plot(plot_type='dist', x=rating_mean_df, xlabel='# of Followers', ylabel='Frequency', title=\"# of Followers Per User\",bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for the average followers count of all users in the dataset\n",
    "followers_mean_df = reviews_df[reviews_df.user_followers_count>=1000].groupby('user_id')['user_followers_count'].mean()\n",
    "sns_plot(plot_type='dist', x=followers_mean_df, xlabel='# of Followers', ylabel='Frequency', title=\"# of Followers > 1000 Per User\",bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap shows there is virtually no correlation between average rating per user and total review count per user, even when setting # of reviews greater than 650\n",
    "sns.heatmap(average_rating_per_user[average_rating_per_user.total_review_count > 650].corr(), annot=True, cmap=\"coolwarm\",fmt='.2f',\n",
    "                 linewidths=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation between a user's # of followers (profiled as overall total in Zomato) and the user's average rating\n",
    "average_rating_follower_per_user = pd.DataFrame(reviews_df.groupby('user_id')['star_rating'].mean())\n",
    "average_rating_follower_per_user['total_follower_count'] = average_rating_follower_per_user.index.map(lambda x: np.max(reviews_df[reviews_df.user_id == x]['user_followers_count']))\n",
    "sns_plot(plot_type='scatter', x=average_rating_follower_per_user.total_follower_count, y=average_rating_follower_per_user.star_rating,\n",
    "         xlabel='Total # of Followers Per User', ylabel='Average Rating', title=\"User Follower Count vs. Average Rating\\nFollower Count - User's Total in Zomato\", figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap shows a slight correlation between star rating and total follower count of a user, even when # of followers is greater than 1000\n",
    "# testing the effect to star ratings provided when the # of followers is high - very slight correlation exists\n",
    "sns.heatmap(average_rating_follower_per_user[average_rating_follower_per_user.total_follower_count > 1000].corr(), annot=True, cmap=\"coolwarm\",fmt='.2f',\n",
    "                 linewidths=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heatmap below shows good correlation between user_review_count and user_follower_count;\n",
    "# it also shows no correlation between star_rating and the each of the former 2 features\n",
    "sns.heatmap(reviews_df[['user_review_count','user_followers_count','star_rating']].corr(), annot=True, cmap=\"coolwarm\",fmt='.2f',\n",
    "                 linewidths=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 2 dataframes by column, retaining the index of the left dataframe\n",
    "def append_dataframes_by_column_retain_left_index(df_left, df_right):\n",
    "\n",
    "    # change right dataframe's index to be the same as the left dataframe\n",
    "    df_right.set_index(df_left.index,inplace=True)\n",
    "    # concatenate the 2 dataframes by the columns\n",
    "    df_left = pd.concat([df_left,df_right],axis=1)\n",
    "\n",
    "    return df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_columns(df, columns_to_dummify_list):\n",
    "\n",
    "    # for every column to dummify\n",
    "    for column in columns_to_dummify_list:\n",
    "\n",
    "        # fit labelbinarizer to job_category_1 column\n",
    "        lb = preprocessing.LabelBinarizer(sparse_output=True)\n",
    "        dummies_sparse = lb.fit_transform(df[column])\n",
    "\n",
    "        # dummify values to a matrix and load on to a dataframe\n",
    "        column_names = [(column + '_' + dummy_class) for dummy_class in lb.classes_]\n",
    "        df_dummies = pd.DataFrame(dummies_sparse.todense(), columns=column_names)\n",
    "        # drop last column for dummies (to dummify - 1)\n",
    "        df_dummies.drop(df_dummies.columns[-1], axis=1, inplace=True)\n",
    "        # drop the original column (the one dummies were based on)\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "        # combine the 2 dataframes, retaining the left dataframe's index\n",
    "        df = append_dataframes_by_column_retain_left_index(df,df_dummies)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing the dataset\n",
    "Trimming the dataset down may be necessary to increase the relevance of the results generated from a recommender system. \n",
    "\n",
    "As an example, if the restaurants were filtered to those that have at least 20 reviews, thus disregarding those that have less than 20 reviews and effectively impacting the number of users and reviews left in the dataset, then the recommender would be able to provide more useful recommendations. These recommendations are ideally calculated with user-item and item-item collaborative filtering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the impact of slicing the dataset by analyzing the resulting number of restaurants, users, and ratings\n",
    "def assess_slice_dataset(minimum_count, slice_on_feature, reviews_df):\n",
    "    \n",
    "    reviews_sliced = reviews_df.copy()\n",
    "\n",
    "    # if slicing is based on the feature restaurant_id\n",
    "    if slice_on_feature == 'restaurant_id':\n",
    "        \n",
    "        # get the count of reviews per restaurant\n",
    "        review_count_per_restaurant = reviews_sliced.groupby('restaurant_id')['restaurant_id'].count()\n",
    "        # filter to restaurants that satisfy the minimum # of reviews requirement\n",
    "        review_count_per_restaurant = review_count_per_restaurant[review_count_per_restaurant>=minimum_count]\n",
    "        # get the restaurant_id's\n",
    "        feature_ids = list(review_count_per_restaurant.index)\n",
    "\n",
    "        # apply filter to dataset\n",
    "        reviews_sliced = reviews_sliced[reviews_sliced.restaurant_id.isin(feature_ids)]\n",
    "\n",
    "        # count # of restaurants\n",
    "        restaurant_count = len(feature_ids)\n",
    "        # count # of users who left review(s) to one or more of these restaurants\n",
    "        user_count = reviews_sliced.user_id.nunique()\n",
    "        \n",
    "    # if slicing is based on the feature user_id\n",
    "    elif slice_on_feature == 'user_id':\n",
    "        # get the count of reviews per user\n",
    "        review_count_per_user = reviews_sliced.groupby('user_id')['user_id'].count()\n",
    "        # filter to users that satisfy the minimum # of reviews requirement\n",
    "        review_count_per_user = review_count_per_user[review_count_per_user>=minimum_count]\n",
    "        # get the user_id's\n",
    "        feature_ids = list(review_count_per_user.index)\n",
    "\n",
    "        # apply filter to dataset\n",
    "        reviews_sliced = reviews_sliced[reviews_sliced.user_id.isin(feature_ids)]    \n",
    "\n",
    "        # count # of restaurants\n",
    "        restaurant_count = reviews_sliced.restaurant_id.nunique()\n",
    "        # count # of users who left review(s) to one or more of these restaurants\n",
    "        user_count = len(feature_ids)\n",
    "    \n",
    "    # count # of reviews for these restaurants\n",
    "    review_count = reviews_sliced.review_id.count()\n",
    "\n",
    "    display('Sliced by ' + slice_on_feature + '; minimum count at ' + str(minimum_count))\n",
    "    display('Remaining # of restaurants: ' + str(restaurant_count))\n",
    "    display('Remaining # of users: ' + str(user_count))\n",
    "    display('Remaining # of reviews: ' + str(review_count))\n",
    "    display('Reviews to matrix size ratio: ' + str(review_count / (restaurant_count * user_count)))\n",
    "    display('-' * 20)\n",
    "    return reviews_sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the dataset with these criteria\n",
    "\n",
    "# slice dataset to filter to restaurants with 25 or more reviews\n",
    "reviews_sliced = assess_slice_dataset(25, 'restaurant_id', reviews_df)\n",
    "# slice dataset to filter to users with 5 or more reviews in the dataset\n",
    "reviews_sliced = assess_slice_dataset(10, 'user_id', reviews_sliced)\n",
    "reviews_sliced = assess_slice_dataset(5, 'restaurant_id', reviews_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the dataset with these criteria\n",
    "\n",
    "#reviews_sliced = assess_slice_dataset(25, 'restaurant_id', reviews_df)\n",
    "reviews_sliced = assess_slice_dataset(10, 'user_id', reviews_df)\n",
    "reviews_sliced = assess_slice_dataset(10, 'restaurant_id', reviews_sliced)\n",
    "reviews_sliced = assess_slice_dataset(5, 'user_id', reviews_sliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Dev-Test split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into to train and test sets\n",
    "train_reviews_df, test_reviews_df = train_test_split(reviews_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# split train further into train and dev sets\n",
    "train_reviews_df, dev_reviews_df = train_test_split(train_reviews_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# display shapes\n",
    "print('Full data : ',  reviews_df.shape)\n",
    "print('Train set : ', train_reviews_df.shape)\n",
    "print('Dev set : ',  dev_reviews_df.shape)\n",
    "print('Test set : ', test_reviews_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total # of unique restaurants, users, and reviews before dataset split\n",
    "stat_before_split = pd.DataFrame([[len(reviews_df.restaurant_id.unique())], [len(reviews_df.user_id.unique())], [reviews_df.review_id.count()]], columns=['Total before Split'], index = ['# of Restaurants', '# of Users', '# of Reviews']).T\n",
    "display(stat_before_split.head())\n",
    "# total # of unique restaurants, users, and reviews for train dataset\n",
    "stat_train = pd.DataFrame([[len(train_reviews_df.restaurant_id.unique())], [len(train_reviews_df.user_id.unique())], [train_reviews_df.review_id.count()]], columns=['Total for Train set'], index = ['# of Restaurants', '# of Users', '# of Reviews']).T\n",
    "display(stat_train.head())\n",
    "# total # of unique restaurants, users, and reviews for dev dataset\n",
    "stat_dev = pd.DataFrame([[len(dev_reviews_df.restaurant_id.unique())], [len(dev_reviews_df.user_id.unique())], [dev_reviews_df.review_id.count()]], columns=['Total for Dev set'], index = ['# of Restaurants', '# of Users', '# of Reviews']).T\n",
    "display(stat_dev.head())\n",
    "# total # of unique restaurants, users, and reviews for test dataset\n",
    "stat_test = pd.DataFrame([[len(test_reviews_df.restaurant_id.unique())], [len(test_reviews_df.user_id.unique())], [test_reviews_df.review_id.count()]], columns=['Total for Test set'], index = ['# of Restaurants', '# of Users', '# of Reviews']).T\n",
    "display(stat_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check sparsity level of the reviews dataset\n",
    "sparsity=round(1.0-len(reviews_df)/float(reviews_df.shape[0]*reviews_df.shape[1]),3)\n",
    "print(\"The sparsity level of Zomato 'Best of Melbourne' main dataset is \" +  str(sparsity*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matrix (user-item-rating) matrix for the main reviews dataframe\n",
    "main_matrix = reviews_df.pivot_table(values='star_rating', columns='restaurant_id', index='user_id').fillna(2.5)\n",
    "main_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare matrices for the recommender system (change null rating to 2.5)\n",
    "main_matrix = reviews_df.pivot_table(values='star_rating', columns='restaurant_id', index='user_id').fillna(2.5)\n",
    "train_matrix = train_reviews_df.pivot_table(values='star_rating', columns='restaurant_id', index='user_id').fillna(2.5)\n",
    "dev_matrix = dev_reviews_df.pivot_table(values='star_rating', columns='restaurant_id', index='user_id').fillna(2.5)\n",
    "test_matrix = test_reviews_df.pivot_table(values='star_rating', columns='restaurant_id', index='user_id').fillna(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_shape_list = [main_matrix.shape,train_matrix.shape,dev_matrix.shape,test_matrix.shape]\n",
    "row_list = []\n",
    "column_list = []\n",
    "for shape_row, shape_column in matrix_shape_list:\n",
    "    row_list.append(shape_row)\n",
    "    column_list.append(shape_column)\n",
    "\n",
    "matrix_shapes_df = pd.DataFrame({'# of Rows':row_list, '# of Columns:':column_list}, index=['Full matrix','Train matrix','Dev matrix','Test matrix'])\n",
    "matrix_shapes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top n restaurants with the most # of m ratings, where m can be a single or a list of ratings \n",
    "def get_n_restaurants_with_m_ratings(assess_reviews_df, restaurant_count, ratings_list):    \n",
    "    top_n_restaurants_with_m_ratings = assess_reviews_df[assess_reviews_df.star_rating.isin(ratings_list)].groupby('restaurant_id')['restaurant_id'].count().sort_values(ascending=False).head(restaurant_count)\n",
    "    top_n_restaurants_df = businesses_df[businesses_df.restaurant_id.isin(top_n_restaurants_with_m_ratings.index[:restaurant_count])][['restaurant_id','name']].set_index('restaurant_id')\n",
    "    top_n_restaurants_df = top_n_restaurants_df.join(top_n_restaurants_with_m_ratings, how='inner')\n",
    "\n",
    "    star_column_label = ''\n",
    "    for rating in ratings_list:\n",
    "        if ratings_list[0]==rating:\n",
    "            star_column_label = str(rating)\n",
    "        else:\n",
    "            star_column_label += '/' + str(rating)\n",
    "    star_column_label += ' Star Rating Count'\n",
    "\n",
    "    top_n_restaurants_df.rename(columns={'name': 'Name', 'restaurant_id': star_column_label},inplace=True)\n",
    "    return top_n_restaurants_df.sort_values(star_column_label, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random restaurant from a restaurants dataframe\n",
    "def get_random_restaurant_from_df(top_n_restaurants_df):\n",
    "    random_index = np.random.choice(np.arange(0, top_n_restaurants_df.shape[0]), size=1)\n",
    "    subject_restaurant = pd.DataFrame(user_item_matrix.loc[:,top_n_restaurants_df.index[random_index[0]]])\n",
    "    return subject_restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------\n",
    "### Personal Favorites-Based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list # of reviews per user (to spot the most active reviewers) in the dataset\n",
    "# choose one user in this list to test the personal favorites-based recommendation\n",
    "reviews_df.groupby('user_id')['restaurant_id'].count().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the subject user's 5-star rated srestaurants, sorted chronologically\n",
    "# performing this against the entire dataset, rather than limiting it to the training set,\n",
    "# for the purpose of maximizing the entire list of reviews of a given user and, thus, yield more insightful results\n",
    "\n",
    "# choose subject user\n",
    "subject_user = user_item_matrix.sample()\n",
    "subject_user.head()\n",
    "# for testing: set subject user to the one with the highest # of reviews in the dataset\n",
    "subject_user = user_item_matrix[user_item_matrix.index==37495749]\n",
    "\n",
    "# get the restaurants rated with 5 stars by subject user\n",
    "personal_favorites_df = reviews_df[(reviews_df.user_id==subject_user.index[0]) & (reviews_df.star_rating.isin([5]))][['restaurant_id','user_id','user_name','review_date','star_rating','textual_review']].sort_values('review_date',ascending=False).set_index('user_id')\n",
    "user_name = personal_favorites_df.iloc[0,1]\n",
    "\n",
    "#businesses_df[['restaurant_id', 'name']].set_index('restaurant_id')\n",
    "\n",
    "personal_favorites_df = personal_favorites_df.merge(businesses_df[['restaurant_id','name']], on='restaurant_id') #left_on=personal_favorites_df.index, right_on='restaurant_id')\n",
    "personal_favorites_df = personal_favorites_df[['review_date','name','star_rating','textual_review']]\n",
    "personal_favorites_df = personal_favorites_df.rename(columns={'review_date':'Review Date', 'name':'Restaurant Name', 'star_rating':'Star Rating', 'textual_review': 'Review'}).set_index('Review Date')\n",
    "\n",
    "# Get the top 10 restaurants in the entire dataset with the most # of 5-Star Ratings\n",
    "\n",
    "display('User Name: ' + str(user_name))\n",
    "display('Personal Favorites: 5 Star Rated Restaurants sorted chronologically (entire dataset):')\n",
    "display(personal_favorites_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------\n",
    "### Popularity-Based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 restaurants in the entire dataset with the most # of 5-Star Ratings\n",
    "restaurant_count=10\n",
    "ratings_list=[5]\n",
    "top_n_restaurants_df = get_n_restaurants_with_m_ratings(reviews_df, restaurant_count, ratings_list)\n",
    "display('Popularity-Based: Top 10 Restaurants with most 5-Star Ratings (entire dataset):')\n",
    "display(top_n_restaurants_df.head(restaurant_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 restaurants in the train set with the most # of 5-Star Ratings\n",
    "restaurant_count=10\n",
    "ratings_list=[5]\n",
    "top_n_restaurants_df = get_n_restaurants_with_m_ratings(train_reviews_df, restaurant_count, ratings_list)\n",
    "display('Popularity-Based: Top 10 Restaurants with most 5-Star Ratings (train set):')\n",
    "display(top_n_restaurants_df.head(restaurant_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 restaurants in the dev set with the most # of 5-Star Ratings\n",
    "restaurant_count=10\n",
    "ratings_list=[5]\n",
    "top_n_restaurants_df = get_n_restaurants_with_m_ratings(dev_reviews_df, restaurant_count, ratings_list)\n",
    "display('Popularity-Based: Top 10 Restaurants with most 5-Star Ratings (dev set):')\n",
    "display(top_n_restaurants_df.head(restaurant_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 restaurants in the test set with the most # of 5-Star Ratings\n",
    "restaurant_count=10\n",
    "ratings_list=[5]\n",
    "top_n_restaurants_df = get_n_restaurants_with_m_ratings(test_reviews_df, restaurant_count, ratings_list)\n",
    "display('Popularity-Based: Top 10 Restaurants with most 5-Star Ratings (test set):')\n",
    "display(top_n_restaurants_df.head(restaurant_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------\n",
    "### Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract location dictionary into separate columns\n",
    "businesses_df['location_address'] = businesses_df['location'].apply(lambda x: x['address'])\n",
    "businesses_df['location_locality'] = businesses_df['location'].apply(lambda x: x['locality'])\n",
    "businesses_df['location_city'] = businesses_df['location'].apply(lambda x: x['city'])\n",
    "businesses_df['location_latitude'] = businesses_df['location'].apply(lambda x: x['latitude'])\n",
    "businesses_df['location_longitude'] = businesses_df['location'].apply(lambda x: x['longitude'])\n",
    "businesses_df['location_zipcode'] = businesses_df['location'].apply(lambda x: x['zipcode'])\n",
    "businesses_df['location_country_id'] = businesses_df['location'].apply(lambda x: x['country_id'])\n",
    "# drop location column\n",
    "businesses_df.drop(columns='location',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the latitude/longitude for a given address\n",
    "def get_lat_long_given_address(current_address):\n",
    "    geolocator = Nominatim(user_agent=\"zmatorecsys\")\n",
    "    location = geolocator.geocode(current_address)\n",
    "    current_lat_long = (location.latitude, location.longitude)\n",
    "    #print(current_lat_long)    \n",
    "    return current_lat_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare distances from current location to each of the restaurants in the given dataframe, get top n closest\n",
    "def get_distance_to_restaurants(assess_businesses_df, current_lat_long):\n",
    "\n",
    "    restaurant_id_list = []\n",
    "    distance_to_restaurants_list = []\n",
    "\n",
    "    # get distance to each restaurant\n",
    "    for key, restaurant in assess_businesses_df.iterrows():\n",
    "        # get geodesic distance between 2 locations    \n",
    "        #print(restaurant)\n",
    "        dest_loc=(restaurant.location_latitude, restaurant.location_longitude)\n",
    "        distance_to_restaurants_list.append(round(geodesic(current_lat_long, dest_loc).kilometers, 2))\n",
    "        restaurant_id_list.append(restaurant.restaurant_id)\n",
    "\n",
    "    # save restaurant_id's and their respective distances to a dataframe\n",
    "    distance_to_restaurants_df = pd.DataFrame({'restaurant_id': restaurant_id_list, 'Distance(km)': distance_to_restaurants_list})\n",
    "    distance_to_restaurants_df = assess_businesses_df[['restaurant_id','name']].merge(distance_to_restaurants_df, left_on='restaurant_id', right_on='restaurant_id')\n",
    "    distance_to_restaurants_df.rename(columns={'name': 'Name'}, inplace=True)\n",
    "    distance_to_restaurants_df.set_index('restaurant_id', inplace=True);\n",
    "    \n",
    "    return distance_to_restaurants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Maps API - get distance, duration of travel, duration of travel with traffic\n",
    "# use Google Maps API sparingly, limit it to the 'last leg' of the distance computation, due to its pay-after-n-calls\n",
    "# once the top results are obtained through geopy's geocode, recompute the Google Map Client's geocode\n",
    "import googlemaps\n",
    "import datetime\n",
    "#api_key=<google-maps-api-here>\n",
    "gm_client = googlemaps.Client(key=api_key)\n",
    "gm_client.distance_matrix(current_lat_long, dest_loc, departure_time=datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Maps API - test only - get lat lon coordinates given the address\n",
    "gm_geocode = gm_client.geocode('45 William St Melbourne')\n",
    "# geocode lat: -37.8185548  lon: 144.9590755\n",
    "print(gm_geocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocoder - test only - get current position's lat and long\n",
    "# location is based on ip - if ip is not static, location may refer back to an exchange/node or address related to the ISP\n",
    "import geocoder\n",
    "g = geocoder.ip('me')\n",
    "print(g.latlng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set current address\n",
    "current_address = \"45 William St Melbourne\"\n",
    "#current_address = \"328 Swanston St, Melbourne\"\n",
    "#current_address = \"330 Collins St Melbourne\"\n",
    "\n",
    "# get current location coordinates\n",
    "current_lat_long = get_lat_long_given_address(current_address)\n",
    "\n",
    "# get distances from current address to all restaurants in the dataset\n",
    "distance_to_restaurants_df = get_distance_to_restaurants(businesses_df, current_lat_long)\n",
    "\n",
    "display('Current Location: ' + current_address)\n",
    "display('Content-Based Filtering: \"Restaurants that are Nearby\"')\n",
    "distance_to_restaurants_df.sort_values('Distance(km)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To-Do: enhance Content-Based Recommendation by incorporating price range  \n",
    "businesses_df[businesses_df.price_range==4][['price_range', 'average_cost_for_two']].sort_values('average_cost_for_two')\n",
    "#businesses_df.price_range, businesses_df.average_cost_for_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------\n",
    "### Memory-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = train_matrix.copy()\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pairwise distances for user-item similarity using cosine as metric\n",
    "user_similarity = pairwise_distances(user_item_matrix, metric='cosine')\n",
    "# get pairwise distances for item-item similarity using cosine as metric\n",
    "item_similarity = pairwise_distances(user_item_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------\n",
    "### User-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# choose subject user\n",
    "subject_user = user_item_matrix.sample()\n",
    "subject_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list the subject user's reviews\n",
    "train_reviews_df[train_reviews_df.user_id == subject_user.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform pairwise distance - subject user and user-item matrix\n",
    "per_user_similarity = pairwise_distances(subject_user.values.reshape(1,-1), user_item_matrix,  metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list and sort (by highest to lowest similarity) the users relative to the subject user\n",
    "per_user_similarity_series = pd.Series(per_user_similarity.flatten(), index=user_item_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean value for each column\n",
    "def get_mean_per_column(x):\n",
    "    # change 2.5 (un-rated) to zero\n",
    "    x.replace(2.5,0, inplace=True)\n",
    "    if (x != 0).sum()>0: \n",
    "        return x.sum()/(x != 0).sum()\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare businesses key columns, for merging with recommender df\n",
    "restaurant_names_df = businesses_df[['restaurant_id','name']]\n",
    "restaurant_names_df['overall_rating'] = businesses_df.user_rating.apply(lambda x: x['aggregate_rating'])\n",
    "restaurant_names_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose top n most similar users to subject user\n",
    "top_similar_users = per_user_similarity_series.sort_values().iloc[:100]\n",
    "\n",
    "# get their restaurant ratings\n",
    "top_similar_users = user_item_matrix[user_item_matrix.index.isin(top_similar_users.index)]\n",
    "\n",
    "# filter to those restaurants that subject user hasn't rated\n",
    "user_item_recommended_restaurants_df = top_similar_users.loc[:, (subject_user == 2.5).values.flatten()]\n",
    "\n",
    "# convert 2.5's to zero and get the mean of the ratings per restaurant that subject user hasn't tried/rated\n",
    "user_item_recommended_restaurants_df = pd.DataFrame(user_item_recommended_restaurants_df.apply(get_mean_per_column), columns=['Average Rating(Similar Users)'])\n",
    "\n",
    "# show top 5 recommended, append restaurant name\n",
    "user_item_recommended_restaurants_df = user_item_recommended_restaurants_df.merge(restaurant_names_df, left_on=user_item_recommended_restaurants_df.index, right_on='restaurant_id')\n",
    "user_item_recommended_restaurants_df.set_index('restaurant_id',inplace=True)\n",
    "user_item_recommended_restaurants_df.index.name = 'User-Item CF: Top 5 Restaurants'\n",
    "user_item_recommended_restaurants_df = user_item_recommended_restaurants_df[['name', 'overall_rating', 'Average Rating(Similar Users)']]\n",
    "user_item_recommended_restaurants_df.rename(columns={'name': 'Name', 'overall_rating': 'Restaurant Overall Rating'}, inplace=True)\n",
    "user_item_recommended_restaurants_df.sort_values('Average Rating(Similar Users)', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose subject user\n",
    "subject_user = user_item_matrix.sample()\n",
    "subject_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews posted, or restaurants rated, by the user\n",
    "train_reviews_df[train_reviews_df.user_id == subject_user.index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get list of restaurants that the subject user rated with 4 stars or 5 stars (basically, its the restaurants the subject user has rated positively)\n",
    "subject_user_high_rated_restaurants = subject_user.T\n",
    "subject_user_high_rated_restaurants = subject_user_high_rated_restaurants[(subject_user_high_rated_restaurants.values==5) | (subject_user_high_rated_restaurants.values==4)]\n",
    "\n",
    "# if user has rated multiple restaurants with a 4-star or a 5-star rating, randomly select one among the list\n",
    "if len(subject_user_high_rated_restaurants) > 1:\n",
    "    # do random choice between the high rated restaurants\n",
    "    print('user has more than 1 high rated restaurant')\n",
    "    random_index = np.random.choice(np.arange(0, len(subject_user_high_rated_restaurants)), size=1)\n",
    "    subject_restaurant = pd.DataFrame(user_item_matrix.loc[:,subject_user_high_rated_restaurants.index[random_index[0]]])\n",
    "    \n",
    "# if user has rated only 1 restaurant with a 4-star or a 5-star rating, choose this 1 restaurant\n",
    "elif len(subject_user_high_rated_restaurants) == 1:\n",
    "    # choose the restaurant as the subject restaurant\n",
    "    print('user has 1 high rated restaurant')\n",
    "    subject_restaurant = pd.DataFrame(user_item_matrix.loc[:,subject_user_high_rated_restaurants.index[0]])\n",
    "    subject_restaurant.head()\n",
    "    \n",
    "# if user hasn't rated any restaurant with a 4-star or a 5-star rating, choose randomly among the most popular restaurants based on other people's ratings\n",
    "elif len(subject_user_high_rated_restaurants) == 0:\n",
    "    # return a random restaurant out of list of most popular restaurants\n",
    "    # get a random restaurant out of the 10 most popular in Zomato's Best of Melbourne' collection \n",
    "    # (most popular in this case refers to the restaurant with the highest # of 4 or 5 star ratings)\n",
    "    print('user has no high rated restaurant')\n",
    "    restaurant_count=10\n",
    "    ratings_list=[4,5]\n",
    "    top_n_restaurants_df = get_n_restaurants_with_m_ratings(train_reviews_df, restaurant_count, ratings_list)\n",
    "    #display(top_n_restaurants_df.head(restaurant_count))\n",
    "    subject_restaurant = get_random_restaurant_from_df(top_n_restaurants_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show subject user's list of restaurants rated as 4 or 5 stars, if there's any\n",
    "subject_user_high_rated_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show subject restaurant to be used for item-item similarity\n",
    "subject_restaurant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of the user's high-rated restaurants, or show the chosen subject restaurant out of the high-rated restaurants by the subject user, if there's any\n",
    "businesses_df[businesses_df.restaurant_id == subject_restaurant.columns[0]][['restaurant_id', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform pairwise distance - subject restaurant and user-item matrix\n",
    "per_item_similarity = pairwise_distances(subject_restaurant.values.reshape(1,-1), user_item_matrix.T,  metric='cosine')\n",
    "per_item_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list and sort by highest to lowest similarity the restaurants relative to the subject restaurant\n",
    "per_item_similarity_series = pd.Series(per_item_similarity.flatten(), index=user_item_matrix.columns)\n",
    "#per_item_similarity_series.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show top 5 recommended restaurants\n",
    "\n",
    "# filter to the restaurants that the subject user has not rated\n",
    "restaurants_not_tried_list = subject_user[subject_user == 2.5].dropna(axis=1).columns\n",
    "item_item_recommended_restaurants_df = pd.DataFrame(round(per_item_similarity_series[per_item_similarity_series.index.isin(restaurants_not_tried_list)], 3), columns=['Similarity (High to Low)'])\n",
    "item_item_recommended_restaurants_df = item_item_recommended_restaurants_df.merge(restaurant_names_df, left_on=item_item_recommended_restaurants_df.index, right_on='restaurant_id')\n",
    "item_item_recommended_restaurants_df.set_index('restaurant_id',inplace=True)\n",
    "item_item_recommended_restaurants_df.index.name ='Item-Item CF: Top 10 Restaurants'\n",
    "item_item_recommended_restaurants_df = item_item_recommended_restaurants_df[['name','overall_rating','Similarity (High to Low)']]\n",
    "item_item_recommended_restaurants_df.rename(columns={'name': 'Name', 'overall_rating': 'Restaurant Overall Rating'}, inplace=True)\n",
    "item_item_recommended_restaurants_df.sort_values('Similarity (High to Low)').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------------------------------------------------------------------------\n",
    "### Hybrid Recommender - User-Item CF & Content- Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Content-Based component: we refer to the location attribute of the restaurants\n",
    "# Collaborative Filtering component: User-Item\n",
    "\n",
    "# set current address\n",
    "current_address = \"45 William St Melbourne\"\n",
    "#current_address = \"328 Swanston St, Melbourne\"\n",
    "#current_address = \"330 Collins St Melbourne\"\n",
    "\n",
    "# get current location coordinates\n",
    "current_lat_long = get_lat_long_given_address(current_address)\n",
    "\n",
    "# format the user-item cf dataframe to calculate distance to restaurants\n",
    "hybrid_user_item_df = user_item_recommended_restaurants_df.merge(businesses_df[['restaurant_id','location_latitude','location_longitude']],\n",
    "                                                       left_on=user_item_recommended_restaurants_df.index, right_on='restaurant_id').rename(columns={'Name': 'name'})\n",
    "\n",
    "# get distances from current address to the top 10 restaurants in the user-item CF dataframe\n",
    "distance_to_restaurants_df = get_distance_to_restaurants(hybrid_user_item_df.sort_values('Average Rating(Similar Users)', ascending=False).head(10), current_lat_long)\n",
    "\n",
    "display('Current Location: ' + current_address)\n",
    "display('Hybrid Recommender (User-Item CF & Content-Based) : \"Restaurants that similar users liked, and are Nearby\"')\n",
    "\n",
    "# merge the User-Item CF dataframe with distance-to-restaurants dataframe to include the relevant columns\n",
    "hybrid_user_item_df = hybrid_user_item_df.set_index('restaurant_id').join(distance_to_restaurants_df, how='inner').drop(columns=['Name','location_latitude','location_longitude'], axis=1)\n",
    "hybrid_user_item_df = hybrid_user_item_df.rename(columns={'name': 'Name'})\n",
    "hybrid_user_item_df.sort_values('Distance(km)').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------------------------------------------------------------------------\n",
    "### Hybrid Recommender - Item-Item CF & Content- Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Based component: we refer to the location attribute of the restaurants\n",
    "# Collaborative Filtering component: Item-Item\n",
    "\n",
    "# set current address\n",
    "current_address = \"45 William St Melbourne\"\n",
    "#current_address = \"328 Swanston St, Melbourne\"\n",
    "#current_address = \"330 Collins St Melbourne\"\n",
    "\n",
    "# get current location coordinates\n",
    "current_lat_long = get_lat_long_given_address(current_address)\n",
    "\n",
    "# format the user-item cf dataframe to calculate distance to restaurants\n",
    "hybrid_item_item_df = item_item_recommended_restaurants_df.merge(businesses_df[['restaurant_id','location_latitude','location_longitude']],\n",
    "                                                       left_on=item_item_recommended_restaurants_df.index, right_on='restaurant_id').rename(columns={'Name': 'name'})\n",
    "\n",
    "# get distances from current address to the top 10 restaurants in the user-item CF dataframe\n",
    "distance_to_restaurants_df = get_distance_to_restaurants(hybrid_item_item_df.sort_values('Similarity (High to Low)').head(10), current_lat_long)\n",
    "\n",
    "display('Current Location: ' + current_address)\n",
    "display('Hybrid Recommender (Item-Item CF & Content-Based) : \"Restaurants that received similar ratings, and are Nearby\"')\n",
    "\n",
    "# merge the User-Item CF dataframe with distance-to-restaurants dataframe to include the relevant columns\n",
    "hybrid_item_item_df = hybrid_item_item_df.set_index('restaurant_id').join(distance_to_restaurants_df, how='inner').drop(columns=['Name','location_latitude','location_longitude'], axis=1)\n",
    "hybrid_item_item_df = hybrid_item_item_df.rename(columns={'name': 'Name'})\n",
    "hybrid_item_item_df.sort_values('Distance(km)').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------------------------------------------------------------------------\n",
    "### User-Item, Item-Item Collaborative Filtering - using pairwise-normalized dot product method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the restaurant_id's and user_id's, to prepare for setting up the matrices for prediction/evaluation\n",
    "main_restaurant_id_list = pd.Series(main_matrix.columns.unique())\n",
    "main_user_id_list = pd.Series(main_matrix.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df[reviews_df.star_rating.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_for_prediction = np.zeros((main_matrix.shape[0], main_matrix.shape[1]))\n",
    "train_matrix_for_prediction[train_matrix_for_prediction == 0] = 2.5\n",
    "for line in train_reviews_df.itertuples():\n",
    "    row_index = main_user_id_list.loc[(main_user_id_list == line[3])].index[0]\n",
    "    column_index = main_restaurant_id_list.loc[(main_restaurant_id_list == line[2])].index[0]\n",
    "    train_matrix_for_prediction[row_index, column_index] = line[8]\n",
    "\n",
    "dev_matrix_for_prediction = np.zeros((main_matrix.shape[0], main_matrix.shape[1]))\n",
    "dev_matrix_for_prediction[dev_matrix_for_prediction == 0] = 2.5\n",
    "for line in dev_reviews_df.itertuples():\n",
    "    row_index = main_user_id_list.loc[(main_user_id_list == line[3])].index[0]\n",
    "    column_index = main_restaurant_id_list.loc[(main_restaurant_id_list == line[2])].index[0]\n",
    "    dev_matrix_for_prediction[row_index, column_index] = line[8]  \n",
    "\n",
    "test_matrix_for_prediction = np.zeros((main_matrix.shape[0], main_matrix.shape[1]))\n",
    "test_matrix_for_prediction[test_matrix_for_prediction == 0] = 2.5\n",
    "for line in test_reviews_df.itertuples():\n",
    "    row_index = main_user_id_list.loc[(main_user_id_list == line[3])].index[0]\n",
    "    column_index = main_restaurant_id_list.loc[(main_restaurant_id_list == line[2])].index[0]\n",
    "    test_matrix_for_prediction[row_index, column_index] = line[8]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy matrix with extreme values, to test prediction against train matrix' cosine similarity, as well as RMSE evaluation\n",
    "dummy_matrix_for_prediction = np.zeros((main_matrix.shape[0], main_matrix.shape[1]))\n",
    "dummy_matrix_for_prediction[dummy_matrix_for_prediction == 0] = 10\n",
    "for line in train_reviews_df.itertuples():\n",
    "    row_index = main_user_id_list.loc[(main_user_id_list == line[3])].index[0]\n",
    "    column_index = main_restaurant_id_list.loc[(main_restaurant_id_list == line[2])].index[0]\n",
    "    dummy_matrix_for_prediction[row_index, column_index] = line[8] + 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_matrix_for_prediction, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_matrix_for_prediction.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict rating for a matrix or an indidivual user or restaurant\n",
    "def make_prediction(ratings, similarity, cf_type='user', original_matrix_shape=None):\n",
    "    # if given rating is of an individual user\n",
    "    if cf_type == 'user' and ratings.shape[0]==1:\n",
    "        print('user, ==1')\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)]) \n",
    "    # if given rating is of a matrix\n",
    "    elif cf_type == 'user' and ratings.shape[0]>1:\n",
    "        print('user, >1')\n",
    "        # check if rating matrix shape is identical to that of the similarity matrix\n",
    "        if ratings.shape[0] == original_matrix_shape[0] and ratings.shape[1] == original_matrix_shape[1]:\n",
    "            mean_user_rating = ratings.mean(axis=1)\n",
    "            ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "            pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "        else:\n",
    "            # make a matrix that is identical in shape to similarity matrix\n",
    "            pass    \n",
    "    elif cf_type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])     \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity.shape, user_similarity.shape, train_matrix_for_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_item_prediction = make_prediction(train_matrix_for_prediction, item_similarity, 'item', train_matrix_for_prediction.shape)\n",
    "train_user_prediction = make_prediction(train_matrix_for_prediction, user_similarity, 'user', train_matrix_for_prediction.shape)\n",
    "#train_item_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train_item_prediction = make_prediction(train_matrix_for_prediction[0], item_similarity, 'item', train_matrix_for_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(single_train_item_prediction, train_item_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_for_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_matrix_for_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matrix_for_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_item_prediction = make_prediction(dev_matrix_for_prediction, item_similarity, 'item', train_matrix_for_prediction.shape)\n",
    "dev_user_prediction = make_prediction(dev_matrix_for_prediction, user_similarity, 'user',train_matrix_for_prediction.shape)\n",
    "dev_item_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_prediction = make_prediction(test_matrix_for_prediction, item_similarity, 'item', train_matrix_for_prediction.shape)\n",
    "test_user_prediction = make_prediction(test_matrix_for_prediction, user_similarity, 'user', train_matrix_for_prediction.shape)\n",
    "test_item_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_item_prediction = make_prediction(dummy_matrix_for_prediction, item_similarity, 'item', train_matrix_for_prediction.shape)\n",
    "dummy_user_prediction = make_prediction(dummy_matrix_for_prediction, user_similarity, 'user', train_matrix_for_prediction.shape)\n",
    "dummy_item_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform evaluation of prediction through RMSE\n",
    "def rmse(prediction, ground_truth):\n",
    "#    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "#    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Set Evaluation:')\n",
    "print('User-based CF RMSE: ' + str(rmse(train_user_prediction, train_matrix_for_prediction)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(train_item_prediction, train_matrix_for_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nDev Set Evaluation:\\n')\n",
    "print('User-based CF RMSE: ' + str(rmse(dev_user_prediction, dev_matrix_for_prediction)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(dev_item_prediction, dev_matrix_for_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nTest Set Evaluation:\\n')\n",
    "print('User-based CF RMSE: ' + str(rmse(test_user_prediction, test_matrix_for_prediction)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(test_item_prediction, test_matrix_for_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nDummy Set Evaluation:\\n')\n",
    "print('User-based CF RMSE: ' + str(rmse(dummy_user_prediction, dummy_matrix_for_prediction)))\n",
    "print('Item-based CF RMSE: ' + str(rmse(dummy_item_prediction, dummy_matrix_for_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------------------------------------------------------------------------\n",
    "### Matrix Factorization - Singular Value Decomposition (SVD) - using scipy's svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_matrix_for_prediction, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print('User-based CF - MSE - train matrix: ' + str(rmse(X_pred, train_matrix_for_prediction)))\n",
    "print('User-based CF - MSE - dev matrix: ' + str(rmse(X_pred, dev_matrix_for_prediction)))\n",
    "print('User-based CF - MSE - test matrix: ' + str(rmse(X_pred, test_matrix_for_prediction)))\n",
    "print('User-based CF - MSE - dummy matrix: ' + str(rmse(X_pred, dummy_matrix_for_prediction)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------------------------------------------------------------------------------------------------------\n",
    "### Matrix Factorization - Singular Value Decomposition (SVD) - using Surpriselib SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate, train_test_split\n",
    "\n",
    "# setup Reader - range of ratings\n",
    "reader = Reader(rating_scale=(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format df's as required by Surprise SVD\n",
    "svd_train_reviews_df = train_reviews_df[['user_id','restaurant_id','star_rating']]\n",
    "svd_dev_reviews_df = dev_reviews_df[['user_id','restaurant_id','star_rating']]\n",
    "svd_test_reviews_df = test_reviews_df[['user_id','restaurant_id','star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataset through Surprise's Dataset module - to prepare it for Surprise's train_test_split\n",
    "# The DataFrame needs to have 3 columns in this specific order: [user_id, product_id, rating]\n",
    "data = Dataset.load_from_df(svd_train_reviews_df, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data through Surprise's train_test_split (temporarily set split size to .01 as this set has already been previously split as a train set)\n",
    "trainset, testset = train_test_split(data, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train SVD with random value: 100 (latent features)\n",
    "model = SVD(n_factors=100)\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "display('before normalized: ', pd.DataFrame(model.qi).iloc[0].pow(2).sum())\n",
    "model.qi /= np.linalg.norm(model.qi, ord=2, axis=1).reshape(-1, 1)\n",
    "display('after normalized: ', pd.DataFrame(model.qi).iloc[0].pow(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view restaurants with highest (or lowest) average ratings\n",
    "train_reviews_df.groupby('restaurant_id')['star_rating'].mean().sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list star ratings for a speific restaurant\n",
    "train_reviews_df[train_reviews_df.restaurant_id==16579337]['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the rating an existing user would give a restaurant that the user hasn't rated\n",
    "a_user = 28165833\n",
    "a_product = 16571144\n",
    "print(model.predict(a_user, a_product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the rating every user would give a particular restaurant\n",
    "for index, user in train_reviews_df.user_id.iloc[:20].iteritems():\n",
    "    a_product = 16571144\n",
    "    print(type(user))\n",
    "    print(model.predict(user, a_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
